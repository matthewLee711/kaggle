{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the American Community\n",
    "\n",
    "__Matthew Lee__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "The American Community dataset is a survey conducted by the US Census Bureau to provide reasearchers information about the US population. Laws and policies are developed to benefit the majority of citizens within the United States. Since developing policies require not only a very extensive amount of time and money, and . Decisions need to be made with upmost accuracy based on both seen and unseen trends. This \n",
    "\n",
    "\n",
    "## Business Understanding \n",
    "\n",
    "1. there must be 10 or more attributes for analyzing the data (both continuous and discrete), \n",
    "2. there must be at least 30,000 records to classify in the data\n",
    "3. there must be a good documentation of the dataset attributes.\n",
    "\n",
    "[10 points] Give an overview of the dataset and the analyses you will be performing. What is your plan for analyzing the data and why? This section is easiest to write as a planning phase for the assignment. \n",
    "\n",
    "[10 points] Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). Why is this data important and how will you know if you have mined useful knowledge from the dataset? How would you measure the effectiveness of a good learning algorithm in terms of its business use? Be specific.\n",
    "\n",
    "What does this tell us American Society? Our life style. best places to live? crime rates to good living?\n",
    "Can you determine a person by their weight?\n",
    "Is there really such thing as the american dream? -- school, income, WAGP\n",
    "\n",
    "## Dataset Explanation\n",
    "\n",
    "The American Community Dataset is \n",
    "\n",
    "## Business Understanding \n",
    "\n",
    "In this dataset, there are 283 attributes for over 1.6 million rows (people surveyed); actually it is about 6.4 million if you consider there are four files. The objective of this analysis is to provide a good understanding of how much financial success immigrants have in the United States and thus provide a reason or not for politicians to gear more lenient policies towards for example, the health care system or social security to immigrants who are or are not citizens. \n",
    "\n",
    "### Financial Success of Immigrants \n",
    "\n",
    "The benefit of being born an American citizen allows that person to be directly integrated into society, thus allowing that person to understand social norms. Immigrants on the other hand, once they arrive in the United States, they have to integrate into society, which inheritantly gives them a slight disadvantage in terms of being able to compete for schools and job positions. There are many factors which contribute to landing a good job. The method of achieving success will be examined by comparing successful immigrants to less successful one, in order to find attributes that help one become more successful.\n",
    "\n",
    "### \n",
    "This can range from location they live at, level of education they achieved, or even where they immigrated from. There are various factors are which contribute to the successful immigrant. Ultimately, the goal is to analyze and figure out specific traits which contribute to success.\n",
    "\n",
    "## Purpose of data\n",
    "\n",
    "Understanding the American populace is important to allow policy makers to create meaningful laws which will impact the US population the best. More importantly, enacting policies such as social security ...... or a health care system involves from millions to billions of dollars. This dataset provides researchers the ability to create statistical models which will can be used by politicians to influence their political stance when making decisions.\n",
    "\n",
    "### Immigrants metrics (find better word? \n",
    "There are a lot values which can determine the success of a immigrant from one country in comparison to an immigrant from another country. Such influences may include culture, background, socio-economic standing. These are values are not provided in the dataset. However, by searching for basic trends or creating statistical models, this will identify nations that produce immigrants with the highest chance of success in the United States. Further analysis of these nations can be observed in comparison to the United States.\n",
    "\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding\n",
    "[10 points] Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file. \n",
    "\n",
    "Here are the definitions to important metrics that will be used for analysis:\n",
    "\n",
    "Nominal:(no order) ID numbers, eye color, zipcodes | distincness, = not equal (one hot encoded)\n",
    "Ordinal: rankings, grades, height | greater than (integer)\n",
    "Continuous:(floating), temperatures | plus minus (float)\n",
    "\n",
    "__PINCP__ (ordinal) - Person's total income O\n",
    "\n",
    "__WAGP__ (ordinal) - Wage or salary in past 12 months O\n",
    "\n",
    "__SCHL__ (nominal) - Level of education N\n",
    "\n",
    "__ESR__ (nominal) - Employment state record N\n",
    "\n",
    "__CIT__ (nominal) - Citizenship status N\n",
    "           1 .Born in the U.S.\n",
    "           2 .Born in Puerto Rico, Guam, the U.S. Virgin Islands,\n",
    "             .or the Northern Marianas\n",
    "           3 .Born abroad of American parent(s)\n",
    "           4 .U.S. citizen by naturalization\n",
    "           5 .Not a citizen of the U.S.\n",
    "\n",
    "__AGEP__ (ordinal) - Age of person O\n",
    "\n",
    "__ST__ (nominal) - State Code N\n",
    "\n",
    "__MARHDP__ (nominal) - Divorced in past 12 months N\n",
    "\n",
    "__FMARHDP__ (nominal) - Divorced in past 12 months \n",
    "\n",
    "__MARHT__ (ordinal) - Number of times married O\n",
    "\n",
    "__WKL__ (nominal) - when last worked\n",
    "\n",
    "__FOD1P__ (nominal) - field of degree\n",
    "\n",
    "__LANP__ (nominal) - Language spoken at home\n",
    "\n",
    "__MIGPUMA__ - migration puma\n",
    "\n",
    "__MIGSP__ - Migration record: need to remove states\n",
    "\n",
    "__MSP__ (nominal) - Married, spouse present/spouse absent\n",
    "\n",
    "__NAICSP__ (nominal) - Job the individual has\n",
    "\n",
    "__NATIVITY__ (nominal) - Born in america or not in america \n",
    "\n",
    "__OC__ (nominal) - own child\n",
    "\n",
    "__POVIP__ (ordinal) - Poverty to income ratio (housing)\n",
    "\n",
    "__POWSP__ (nominal) - Place of work (state)\n",
    "\n",
    "__RAC1P__ (nominal) - Recorded detailed race code\n",
    "           1 .White alone                             \n",
    "           2 .Black or African American alone         \n",
    "           3 .American Indian alone                   \n",
    "           4 .Alaska Native alone                     \n",
    "           5 .American Indian and Alaska Native tribes specified; or American\n",
    "             .Indian or Alaska Native, not specified and no other races\n",
    "           6 .Asian alone                             \n",
    "           7 .Native Hawaiian and Other Pacific Islander alone\n",
    "           8 .Some Other Race alone                   \n",
    "           9 .Two or More Races \n",
    "\n",
    "__ENG__ (nominal) - Ability to speak english b .N/A (less than 5 years old/speaks only English)\n",
    "           1 .Very well\n",
    "           2 .Well\n",
    "           3 .Not well\n",
    "           4 .Not at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[15 points] Verify data quality: Explain any missing values, duplicate data, and outliers. Are those mistakes? How do you deal with these problems? Give justifications for your methods (elimination or imputation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Time for data extraction and refinement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Attribute Removal\n",
    "There are quite a few attributes that are not going to be used, so they will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries for data modfication\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load in all the data\n",
    "housingA = pd.read_csv(\"pums/ss13husa.csv\")\n",
    "#housingB = pd.read_csv(\"pums/ss13husb.csv\")\n",
    "individualsA = pd.read_csv(\"pums/ss13pusa.csv\")\n",
    "individualsB = pd.read_csv(\"pums/ss13pusb.csv\")\n",
    "\n",
    "# Combine seperate data into one data frame\n",
    "individualdfs = [individualsA, individualsB]\n",
    "individualdf = pd.concat(individualdfs)\n",
    "\n",
    "# Delete unused data attibute columns\n",
    "delete_columns_housing = ['']\n",
    "delete_columns_individuals = ['pwgtp1', 'pwgtp2', 'pwgtp3', 'pwgtp4', 'pwgtp5', 'pwgtp6', 'pwgtp7', 'pwgtp8', 'pwgtp9', 'pwgtp10',\n",
    "                    'pwgtp11', 'pwgtp12', 'pwgtp13', 'pwgtp14', 'pwgtp15', 'pwgtp16', 'pwgtp17', 'pwgtp18', 'pwgtp19', 'pwgtp20',\n",
    "                    'pwgtp21', 'pwgtp22', 'pwgtp23', 'pwgtp24', 'pwgtp25', 'pwgtp26', 'pwgtp27', 'pwgtp28', 'pwgtp29', 'pwgtp30',\n",
    "                    'pwgtp31', 'pwgtp32', 'pwgtp33', 'pwgtp34', 'pwgtp35', 'pwgtp36', 'pwgtp37', 'pwgtp38', 'pwgtp39', 'pwgtp40',\n",
    "                    'pwgtp41', 'pwgtp42', 'pwgtp43', 'pwgtp44', 'pwgtp45', 'pwgtp46', 'pwgtp47', 'pwgtp48', 'pwgtp49', 'pwgtp50',\n",
    "                    'pwgtp51', 'pwgtp52', 'pwgtp53', 'pwgtp54', 'pwgtp55', 'pwgtp56', 'pwgtp57', 'pwgtp58', 'pwgtp59', 'pwgtp60',\n",
    "                    'pwgtp61', 'pwgtp62', 'pwgtp63', 'pwgtp64', 'pwgtp65', 'pwgtp66', 'pwgtp67', 'pwgtp68', 'pwgtp69', 'pwgtp70',\n",
    "                    'pwgtp71', 'pwgtp72', 'pwgtp73', 'pwgtp74', 'pwgtp75', 'pwgtp76', 'pwgtp77', 'pwgtp78', 'pwgtp79', 'pwgtp80']\n",
    "\n",
    "# delete all columns from certain number?\n",
    "individualdf = individualdf.drop(delete_columns_individuals, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality of Data and Feature Discretization\n",
    "\n",
    "The data attributes will be labeled as continuous, ordinal, or categorical. this will help identify invalid or missing data.\n",
    "\n",
    "Nominal:(no order) ID numbers, eye color, zipcodes | distincness, = not equal (one hot encoded)\n",
    "\n",
    "Ordinal: rankings, grades, height | greater than (integer)\n",
    "\n",
    "Interval:(floating), temperatures | plus minus (float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature Discretization\n",
    "# continuous_features = []\n",
    "ordinal_features = ['PINCP', 'WAGP', 'MARHT', 'AGEP'] #'POVIP', 'MARHDP'\n",
    "nominal_features = ['SCHL', 'ESR', 'CIT', 'WKL', 'MSP', 'NAICSP', 'POWSP', 'ST'] #'FOD1P', 'LANP', 'ENG'\n",
    "binary_features = ['FMARHDP', 'NATIVITY', 'OC'] #'OC'\n",
    "categorical_features = individualdf.columns.difference(nominal_features).difference(ordinal_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "missing values, duplicate data, and outliers. Are those mistakes? How do you deal with these problems? Give justifications for your methods (elimination or imputation). \n",
    "\n",
    "### Missing Information for Continuous Features\n",
    "Continuous feature columns missing information: POWSP (Place of work) and NAICSP (Job held by individual)\n",
    "POWSP has the value NaN filled in when the current individual is not working anywhere. NaN will be imputed with -1 to signify that the individual is not working and the data will not be ignored. NAICSP, similar to POWSP has NaN to signify that the individual is currently not employed. NaN will be imputed with -1 so the data will be given value. ESR (Employment state) does provide whether or not the individual is employed, this attribute will be used to check if the individual misinput the value for POWSP or NAICSP. Data with conflicting ESR and POWSP and NAICSP will be excluded from the dataset to provide better accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Impute NAICSP's and POWSP's NaN with -1\n",
    "individualdf[nominal_features].replace(to_replace=np.nan,value=-1)\n",
    "# Check and remove data conflicts with ESR and POWSP and NAICSP\n",
    "# print individualdf.at[0, 'ESR']\n",
    "# for row in range(0, len(individualdf[nominal_features].axes[0]):\n",
    "#     if individualdf[nominal_features].iat[row, 1] != 3 and (individualdf[nominal_features].iat[row, 1] != -1] or individualdf[nominal_features].iat[row, 1] != -1):\n",
    "#         individualdf[nominal_features].drop(row)\n",
    "\n",
    "individualdf[nominal_features].head()\n",
    "individualdf[nominal_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing for Continuous Features\n",
    "We are going to impute with ........."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Information and Imputing for Ordinal Features\n",
    "Ordinal features were examined, NaN is present in these attributes ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "individualdf[ordinal_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing for Continuous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Information and Imputing for Categorical Features\n",
    "In attributes ...... NaN is present, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "individualdf[categorical_features].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing for Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "#housingA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Missing Information and Imputing for Binary Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make parser that explains information OR explain most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Discretization\n",
    "Process of converting or partitioning continuous attributes, features or variables to discretized or nominal attributes/features/variables/intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inclination of Data\n",
    "\n",
    "Majority of the data attributes are chosen under the purpose to extract individuals who recently came to America. The problem with relying on one attribute such as nativity is not enough. Individuals can choose to not reveal the fact they were born in american because of fear of being deported regardless of anonymity. Instead, combining Nativity, MIGSP, and MIGPUMA will establish a dataset of individuals who came to America. All the other attributes, contribute to developing a consensus of whether coming to America is worth it.\n",
    "\n",
    "# Analysis of Nato\n",
    "\n",
    "Nato created a dataset which values the economic strength of different countries. Information from data attributes (\"\"\"\"\"\"\"\") will determine the poverty-to-income. Life in immigrants vs life in previous state. Since the data is anonymized, determining -- we will look at correlation. High immigration rate from specific countries.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TAKES WAY TOO LONG - error\n",
    "#plt.style.use('ggplot')\n",
    "\n",
    "#df_grouped = individualsAshort.groupby(by=['SCHL', 'ESR'])\n",
    "#df_grouped.plot(kind='barh')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "\n",
    "[10 points] Visualize appropriate statistics (e.g., range, mode, mean, median, variance, counts) for a subset of attributes. Describe anything meaningful or potentially interesting. Note: You can also use data from other sources for comparison. Explain why the statistics run are  meaningful for the attribute. \n",
    "\n",
    "[15 points] Visualize relationships between interesting attributes: Look at the attributes via scatter plots, correlation, cross-tabulation, group-wise averages, etc. as appropriate. Explain any interesting relationships. Important: Interpret the implications for each visualization. Explain for each attribute why the chosen visualization is appropriate.\n",
    "\n",
    "[15 points] Identify and explain interesting relationships between features and the class you are trying to predict (i.e., relationships with variables and the target classification).\n",
    "\n",
    "[5 points] Are there other features that could be added to the data or created from existing features?  Which ones? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup Seaborn \n",
    "import seaborn as sns\n",
    "sns.set_palette('muted')\n",
    "\n",
    "# Setup plotly\n",
    "import plotly\n",
    "plotly.offline.init_notebook_mode() \n",
    "\n",
    "# Setup matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Embed figures into Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Use GGPlot style for matplotlib\n",
    "plt.style.use('ggplot')\n",
    "#plt.style.use(['dark_background', 'presentation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Is it worth coming to america\n",
    "### Distribution of Income\n",
    "Income is a very important attribute because it defines the financial stability and success of a person. \n",
    "\n",
    "Income vs out of america\n",
    "\n",
    "Income vs American"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Mean Income of all US citizens: \" + str(individualdf['PINCP'].mean()))\n",
    "print(\"Median Income of all US citizens: \" + str(individualdf['PINCP'].median()))\n",
    "\n",
    "individualdf['PINCP'].plot(kind='box', ylim=[-20, 120000], title = \"Income\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This box plot shows the distribution of income in 2013. What's really interesting about the graph is that the US Census Bureau claims the average income is 52,250. However, based on taking a simple mean and median, the income distribution is skewed by a huge margin of 20000. Considering the number of people taken from this survey, it seems as if only people from the low income bracket (47,248) took the survey. This proves to be a formitable issue in the analysis of this dataset because this is biased towards middle and upper class. Upper class data does not even exist.\n",
    "\n",
    "College degree income average\n",
    "\n",
    "Income vs out of america\n",
    "\n",
    "Income vs American"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics\n",
    "\n",
    "Majority of the data attributes are chosen under the purpose to extract individuals who recently came to America. The problem with relying on one attribute such as NATIVITY (born in US) is not enough. Individuals can choose to not reveal the fact they were born in american because of fear of being deported regardless of anonymity. Instead, combining Nativity, MIGSP, and MIGPUMA will establish a dataset of individuals who came to America. All the other attributes, contribute to developing a consensus of whether coming to America is worth it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Extract rows for immigrants\n",
    "immigrant_params = ['NATIVITY', 'MIGSP', 'MIGPUMA']\n",
    "individualdf[immigrant_params].describe()\n",
    "# for row in range(0, len(individualdf[nominal_features].axes[0]):\n",
    "#     if individualdf[nominal_features].iat[row, 1] != 3 and (individualdf[nominal_features].iat[row, 1] != -1] or individualdf[nominal_features].iat[row, 1] != -1):\n",
    "#         individualdf[nominal_features].drop(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of immigrants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import plotly.plotly as py\n",
    "\n",
    "# df = individualdf[ordinal_features]\n",
    "\n",
    "# for col in df.columns:\n",
    "#     df[col] = df[col].astype(str)\n",
    "\n",
    "# scl = [[0.0, 'rgb(242,240,247)'],[0.2, 'rgb(218,218,235)'],[0.4, 'rgb(188,189,220)'],\\\n",
    "#             [0.6, 'rgb(158,154,200)'],[0.8, 'rgb(117,107,177)'],[1.0, 'rgb(84,39,143)']]\n",
    "# # ['PINCP', 'WAGP', 'MARHT', 'AGEP']\n",
    "# df['text'] = df['state'] + '<br>' +\\\n",
    "#     'PINCP '+df['PINCP']+' Dairy '+df['WAGP']+'<br>'+\\\n",
    "#     'Fruits '+df['MARHT']+' Veggies ' + df['AGEP']\n",
    "    \n",
    "\n",
    "# data = [ dict(\n",
    "#         type='choropleth',\n",
    "#         colorscale = scl,\n",
    "#         autocolorscale = False,\n",
    "#         locations = df['ST'],\n",
    "#         z = df['PINCP'].astype(float),\n",
    "#         locationmode = 'USA-states',\n",
    "#         text = df['text'],\n",
    "#         marker = dict(\n",
    "#             line = dict (\n",
    "#                 color = 'rgb(255,255,255)',\n",
    "#                 width = 2\n",
    "#             ) ),\n",
    "#         colorbar = dict(\n",
    "#             title = \"Millions USD\")\n",
    "#         ) ]\n",
    "\n",
    "# layout = dict(\n",
    "#         title = '2011 US Agriculture Exports by State<br>(Hover for breakdown)',\n",
    "#         geo = dict(\n",
    "#             scope='usa',\n",
    "#             projection=dict( type='albers usa' ),\n",
    "#             showlakes = True,\n",
    "#             lakecolor = 'rgb(255, 255, 255)'),\n",
    "#              )\n",
    "    \n",
    "# fig = dict( data=data, layout=layout )\n",
    "# py.iplot( fig, filename='d3-cloropleth-map' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "\n",
    "implement dimensionality reduction, then visualize and interpret the results. \n",
    "\n",
    "Reducing the number of random variables\n",
    "\n",
    "Kernal PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
